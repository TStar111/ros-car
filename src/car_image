#!/home/mech/miniconda3/envs/melodic36/bin/python

# Note the specification of conda python

import time
import os, sys
import torch
import numpy as np
import cv2

import rospy
from rospy.exceptions import ROSSerializationException
from serial.serialutil import SerialException
from cv_bridge import CvBridge

from maestro.servo import Servo
from car.msg import Event, Status, ServoStatus
from sensor_msgs.msg import Image
from std_msgs.msg import String
from message_filters import ApproximateTimeSynchronizer, Subscriber

# Might need to be careful depending on structure of package
from detect import detect
from YOLOP.lib.models import get_net
from YOLOP.lib.config import cfg

from PID.pid import Follower

# ALternate implementation of cv_bridge
# from cv_bridge.cv_bridge import bgraimgmsg_to_cv2, depthimgmsg_to_cv2


def get_status(servo):
    status = ServoStatus()
    status.position = servo.get_position()
    status.target = servo.target["current"]
    status.acceleration = servo.acceleration

    status.input_rom = servo.rom["input"]
    status.max_rom = servo.rom["max"]
    status.current_rom = servo.rom["current"]
    status.rom_delta = servo.rom["delta"]

    return status


def image_input(bgra_image, depth_image):

    # Processing the image to something that can be passed into a model
    # No longer using cv_bridge because of incompatability with python3
    # cv_image = bridge.imgmsg_to_cv2(image)
                                   
    # This will return an bgr image
    cv_bgrimage = bridge.imgmsg_to_cv2(bgra_image, desired_encoding="rgb8")

    # This will return an indexable depth image
    # Testing has the value at weird ranges. It's definitely not meters or mms but it does output values
    # TODO: Investigate what the values correlate to
    cv_depthimage = bridge.imgmsg_to_cv2(depth_image, desired_encoding="32FC1")


    # det is a tensor object. 2 dimension, rows represent each detection
    # The output looks to be coordinates of upper left and bottom right, confidence, class
    t0 = time.time()
    # Implementation with reduced model, see README.md
    det, lda_seg_mask = detect(model, cv_bgrimage, device, IMG_SIZE, CONF_THRES, IOU_THRES)
    # det, da_seg_mask, ll_seg_mask = detect(model, cv_bgrimage, device, IMG_SIZE, CONF_THRES, IOU_THRES)
    t1 = time.time()
    print("Time elapsed: %f" % (t1 - t0))

    # Goal is the center of the image
    # Tuned to the ouput image size (should be 640) Hardcoded
    goal = 640

    # Identify the closest bounding box in terms of depth
    # num_detect = len(det)
    # print("Num detect: %d" % num_detect)
    # closest = None
    # closest_depth = None
    # if num_detect != 0:
    #     for i in range(num_detect):
    #         x = int((det[i][0] + det[i][3])/2)
    #         y = int((det[i][1] + det[i][4])/2)
    #         curr_depth = cv_depthimage[x][y]
    #         if curr_depth != 0:
    #             if (closest_depth is None) or (curr_depth < closest_depth):
    #                 # Grab their float center value
    #                 closest = ((det[i][0] + det[i][3])/2, (det[i][1] + det[i][4])/2)
    #                 closest_depth = curr_depth

    # # Identify the closest bounding box in terms of box size
    # num_detect = len(det)
    # print("Num detect: %d" % num_detect)
    # closest = None
    # closest_size = None
    # if num_detect != 0:
    #     for i in range(num_detect):
    #         x = int(abs(det[i][0] - det[i][3]))
    #         y = int(abs(det[i][1] - det[i][4]))
    #         curr_size = x * y
    #         if (closest_size is None) or (curr_size > closest_size):
    #             # Grab their float center value
    #             closest = ((det[i][0] + det[i][3])/2, (det[i][1] + det[i][4])/2)
    #             closest_size = curr_size

    # Identify most confident box
    num_detect = len(det)
    print("Num detect: %d" % num_detect)
    closest = None
    if num_detect != 0:
        closest_idx = torch.argmax(det[:, 4])
        closest = (((det[closest_idx][0] + det[closest_idx][3])/2), ((det[closest_idx][1] + det[closest_idx][4])/2))

    # Calculate controls
    try:
        theta, speed = None, None
        if closest is not None:
            closest = (int(closest[0].item()), int(closest[1].item()))
            theta, speed = follower.control(goal, closest[0])
            print(closest)
            print("Goal: %d" % goal)
            print("Curr: %d" % closest[0])
            print("Theta: %f" % theta)
            print("Depth: %f" % (cv_depthimage[closest[0]][closest[1]]))

        if (theta is not None) and (speed is not None):
            # TODO: What conversions do we need to do before passing to SERVOs
            steer.set_target(theta)
        #     throttle.set_target(speed)
    # TODO: This might be unnecessary now that we use the topic "/depth_to_rgb/image_raw"
    except IndexError:
        pass

def bgra_input(bgra_image):
    """
    Processing for only the rgb image.

    Parameters
    ----------
    bgra_image - ROS Image - from camera topic

    Returns
    -------
    Sends controls to the SERVOs
    
    """

    # Processing the image to something that can be passed into a model
    # No longer using cv_bridge because of incompatability with python3
    # cv_image = bridge.imgmsg_to_cv2(image)
                                   
    # This will return an bgr image
    cv_bgrimage = bridge.imgmsg_to_cv2(bgra_image, desired_encoding="rgb8")

    # det is a tensor object. 2 dimension, rows represent each detection
    # The output looks to be coordinates of upper left and bottom right, confidence, class
    t0 = time.time()
    # det, da_seg_mask, ll_seg_mask = detect(model, cv_bgrimage, device, IMG_SIZE, CONF_THRES, IOU_THRES)
    det, da_seg_mask = detect(model, cv_bgrimage, device, IMG_SIZE, CONF_THRES, IOU_THRES)
    t1 = time.time()
    print("Time elapsed: %f" % (t1 - t0))

    # Goal is the center of the image
    # Tuned to the ouput image size (should be 640) Hardcoded
    goal = 640

    # Identify the closest bounding box in terms of depth
    # num_detect = len(det)
    # print("Num detect: %d" % num_detect)
    # closest = None
    # closest_depth = None
    # if num_detect != 0:
    #     for i in range(num_detect):
    #         x = int((det[i][0] + det[i][3])/2)
    #         y = int((det[i][1] + det[i][4])/2)
    #         curr_depth = cv_depthimage[x][y]
    #         if curr_depth != 0:
    #             if (closest_depth is None) or (curr_depth < closest_depth):
    #                 # Grab their float center value
    #                 closest = ((det[i][0] + det[i][3])/2, (det[i][1] + det[i][4])/2)
    #                 closest_depth = curr_depth

    # # Identify the closest bounding box in terms of box size
    # num_detect = len(det)
    # print("Num detect: %d" % num_detect)
    # closest = None
    # closest_size = None
    # if num_detect != 0:
    #     for i in range(num_detect):
    #         x = int(abs(det[i][0] - det[i][3]))
    #         y = int(abs(det[i][1] - det[i][4]))
    #         curr_size = x * y
    #         if (closest_size is None) or (curr_size > closest_size):
    #             # Grab their float center value
    #             closest = ((det[i][0] + det[i][3])/2, (det[i][1] + det[i][4])/2)
    #             closest_size = curr_size

    # Identify most confident box
    num_detect = len(det)
    print("Num detect: %d" % num_detect)
    closest = None
    if num_detect != 0:
        closest_idx = torch.argmax(det[:, 4])
        closest = ((det[closest_idx][0] + det[closest_idx][3])/2, (det[closest_idx][1] + det[closest_idx][4])/2)

    # Calculate controls
    theta, speed = None, None
    if closest is not None:
        theta, speed = follower.control(goal, closest[0].item())
        print("Goal: %d" % goal)
        print("Curr: %d" % closest[0].item())
        print("Theta: %f" % theta)

    if (theta is not None) and (speed is not None):
        # TODO: What conversions do we need to do before passing to SERVOs
        steer.set_target(theta)
    #     throttle.set_target(speed)


def file_input(string):
    # Testing to see if the model works with default images
    # Note that this will not be used at runtime and should probably be commented out

    # Read the input image
    cv_image = cv2.imread(string.data)

    # TODO: Lowkey I don't think this is necessary?
    # Convert from BGR --> RGB
    # cv_image = cv_image[...,::-1]
    
    # Run inference
    det, da_seg_mask, ll_seg_mask = detect(model, cv_image, device, IMG_SIZE, CONF_THRES, IOU_THRES)

    # See output
    print(det)

def main():

    print()
    print("CAR SETTINGS")
    print("--------------------------------")
    print(f"FPS:                {FPS}")
    print(f"INPUT ROM:          {INPUT_ROM}")
    print(f"SERVO ROM:          {SERVO_ROM}")
    print(f"ACCELERATION DELTA: {ACCELERATION_DELTA}")
    print(f"SROM DELTA:         {SROM_DELTA}")
    print()
    print("DEAD MAN'S SWITCH:  {}".format("ON" if ENABLE_DMS else "OFF"))
    print()

    rate = rospy.Rate(FPS)
    while not rospy.is_shutdown():
        try:
            status = Status()
            status.throttle = get_status(throttle)
            status.steer = get_status(steer)
            status_publisher.publish(status)
            # throttle_publisher.publish(get_status(throttle))
            # steer_publisher.publish(get_status(steer))
            rate.sleep()
        except SerialException:
            pass
        except ROSSerializationException:
            pass
    
    steer.set_target(0)
    throttle.set_target(0)
    # vid_path, vid_writer = None, None

if __name__ == '__main__':
    rospy.init_node('car')
    # Currently set for not using depth camera
    # TODO: Implement flexible design with ROS parameters
    rospy.Subscriber("/rgb/image_raw", Image, bgra_input, queue_size=1)
    rospy.Subscriber("/test", String, file_input, queue_size=1)

    # # Create subscribers for the RGB (actually bgra) and depth image topics
    # rgb_sub = Subscriber('/rgb/image_raw', Image)
    # depth_sub = Subscriber('/depth_to_rgb/image_raw', Image)

    # # Synchronize the RGB and depth image messages
    # sync = ApproximateTimeSynchronizer([rgb_sub, depth_sub], queue_size=10, slop=0.1)
    # sync.registerCallback(image_input)

    status_publisher = rospy.Publisher('/car/status', Status, queue_size=1)

    # Car parameters
    FPS = rospy.get_param('~FPS', 30)
    INPUT_ROM = rospy.get_param('~INPUT_ROM', [-640, 640])
    SERVO_ROM = rospy.get_param('~SERVO_ROM', [4000, 8000])
    ACCELERATION_DELTA = rospy.get_param('~ACCELERATION_DELTA', 1)
    SROM_DELTA = rospy.get_param('~SROM_DELTA', 0.025)
    ENABLE_DMS = rospy.get_param('~ENABLE_DMS', False) # Modified

    # YOLOP parameters
    WEIGHTS = rospy.get_param('~WEIGHTS', '/home/mech/catkin_ws/src/car/src/YOLOP/weights/End-to-end.pth')
    IMG_SIZE = rospy.get_param('~IMG_SIZE', 640)
    CONF_THRES = rospy.get_param('~CONF_THRES', 0.25)
    IOU_THRES = rospy.get_param('~IOU_THRES', 0.45)

    # PID parameters
    P = rospy.get_param('~P', 0.5)
    I = rospy.get_param('~I', 0)
    D = rospy.get_param('~D', 0.05)
    RATE = rospy.get_param('~RATE', 0.01)

    # Initializing the CvBridge with python3 build
    bridge = CvBridge()

    # Initialize PID Controller
    follower = Follower(P, I, D, RATE)
    
    print("cuda available: ", end="")
    print(torch.cuda.is_available())
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
    else:
        device = torch.device("cpu")
    half = False

    # Load net and weights
    model = get_net(cfg)
    checkpoint = torch.load(WEIGHTS, map_location= device)

    # Cleans up unused weights
    unwanted_weights = ["model.34.conv.weight", "model.34.bn.weight", "model.34.bn.bias", "model.34.bn.running_mean", "model.34.bn.running_var", "model.34.bn.num_batches_tracked", "model.36.cv1.conv.weight", "model.36.cv1.bn.weight", "model.36.cv1.bn.bias", "model.36.cv1.bn.running_mean", "model.36.cv1.bn.running_var", "model.36.cv1.bn.num_batches_tracked", "model.36.cv2.weight", "model.36.cv3.weight", "model.36.cv4.conv.weight", "model.36.cv4.bn.weight", "model.36.cv4.bn.bias", "model.36.cv4.bn.running_mean", "model.36.cv4.bn.running_var", "model.36.cv4.bn.num_batches_tracked", "model.36.bn.weight", "model.36.bn.bias", "model.36.bn.running_mean", "model.36.bn.running_var", "model.36.bn.num_batches_tracked", "model.36.m.0.cv1.conv.weight", "model.36.m.0.cv1.bn.weight", "model.36.m.0.cv1.bn.bias", "model.36.m.0.cv1.bn.running_mean", "model.36.m.0.cv1.bn.running_var", "model.36.m.0.cv1.bn.num_batches_tracked", "model.36.m.0.cv2.conv.weight", "model.36.m.0.cv2.bn.weight", "model.36.m.0.cv2.bn.bias", "model.36.m.0.cv2.bn.running_mean", "model.36.m.0.cv2.bn.running_var", "model.36.m.0.cv2.bn.num_batches_tracked", "model.37.conv.weight", "model.37.bn.weight", "model.37.bn.bias", "model.37.bn.running_mean", "model.37.bn.running_var", "model.37.bn.num_batches_tracked", "model.39.conv.weight", "model.39.bn.weight", "model.39.bn.bias", "model.39.bn.running_mean", "model.39.bn.running_var", "model.39.bn.num_batches_tracked", "model.40.cv1.conv.weight", "model.40.cv1.bn.weight", "model.40.cv1.bn.bias", "model.40.cv1.bn.running_mean", "model.40.cv1.bn.running_var", "model.40.cv1.bn.num_batches_tracked", "model.40.cv2.weight", "model.40.cv3.weight", "model.40.cv4.conv.weight", "model.40.cv4.bn.weight", "model.40.cv4.bn.bias", "model.40.cv4.bn.running_mean", "model.40.cv4.bn.running_var", "model.40.cv4.bn.num_batches_tracked", "model.40.bn.weight", "model.40.bn.bias", "model.40.bn.running_mean", "model.40.bn.running_var", "model.40.bn.num_batches_tracked", "model.40.m.0.cv1.conv.weight", "model.40.m.0.cv1.bn.weight", "model.40.m.0.cv1.bn.bias", "model.40.m.0.cv1.bn.running_mean", "model.40.m.0.cv1.bn.running_var", "model.40.m.0.cv1.bn.num_batches_tracked", "model.40.m.0.cv2.conv.weight", "model.40.m.0.cv2.bn.weight", "model.40.m.0.cv2.bn.bias", "model.40.m.0.cv2.bn.running_mean", "model.40.m.0.cv2.bn.running_var", "model.40.m.0.cv2.bn.num_batches_tracked", "model.42.conv.weight", "model.42.bn.weight", "model.42.bn.bias", "model.42.bn.running_mean", "model.42.bn.running_var", "model.42.bn.num_batches_tracked"]
    pretrained_model_state_dict = {k:v for k, v in checkpoint['state_dict'].items() if k not in unwanted_weights}

    # For if no weights are to be removed
    # model.load_state_dict(checkpoint['state_dict'])

    # Load model
    model.load_state_dict(pretrained_model_state_dict)
    model = model.to(device)
    if half:
        model.half()  # to FP16

    # Running a dummy input # TODO: Is this necessary? YOLOP code does this
    img = torch.zeros((1, 3, IMG_SIZE, IMG_SIZE), device=device)  # init img
    _ = model(img.half() if half else img) if device != 'cpu' else None  # run once
    model.eval()

    # Initializing SERVOs
    throttle = Servo(channel=1, irom=INPUT_ROM, srom=SERVO_ROM)
    # throttle.set_srom(delta=.25)
    throttle.set_acceleration(256) # max

    steer = Servo(channel=0, irom=INPUT_ROM, srom=SERVO_ROM)
    steer.set_acceleration(256) # max

    disabled = ENABLE_DMS

    throttle.set_target(0)
    steer.set_target(0)
    main()
    